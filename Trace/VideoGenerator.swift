import Foundation
import AVFoundation
import AppKit
import SwiftUI

class VideoGenerator: NSObject, NSSpeechSynthesizerDelegate {
    static let shared = VideoGenerator()
    
    // å…¨å±€å¤ç”¨è¿™ä¸€ä¸ªåˆæˆå™¨
    private let synthesizer = NSSpeechSynthesizer()
    private var audioContinuation: CheckedContinuation<Void, Never>?
    
    override init() {
        super.init()
        synthesizer.delegate = self
    }
    
    // ä¸»å…¥å£ï¼šç”Ÿæˆè§†é¢‘
    func createTutorialVideo(tutorial: Tutorial, script: [String], completion: @escaping (URL?) -> Void) {
        let steps = tutorial.sortedSteps
        let count = min(steps.count, script.count)
        
        guard count > 0 else {
            print("âŒ Empty step")
            completion(nil)
            return
        }
        
        Task {
            print("ğŸ™ï¸ Creating audio...")
            var audioAssets: [URL] = []
            var durations: [Double] = []
            
            // 1. ä¸²è¡Œç”ŸæˆéŸ³é¢‘
            for i in 0..<count {
                let text = script[i]
                let fileURL = FileManager.default.temporaryDirectory.appendingPathComponent("trace_audio_\(i)_\(UUID().uuidString).aiff")
                
                // ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
                await generateAudioSerial(text: text, to: fileURL)
                
                // è®¡ç®—å‡†ç¡®çš„â€œå£°ç”»åŒæ­¥â€æ—¶é•¿
                let asset = AVURLAsset(url: fileURL)
                if let rawAudioDuration = try? await asset.load(.duration).seconds, rawAudioDuration > 0 {
                    
                    // ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼š
                    // 1. åŸºç¡€æ—¶é•¿ = éŸ³é¢‘æ—¶é•¿ + 1.0ç§’ç¼“å†² (Wait 1s)
                    let targetDuration = rawAudioDuration + 1.0
                    
                    // 2. å¸§ç‡å¯¹é½ï¼šå°†æ—¶é•¿å‘ä¸Šå–æ•´åˆ°æœ€è¿‘çš„å¸§ (30fps)
                    // é¿å…å› ä¸º Double è½¬ Int é€ æˆçš„ç”»é¢æ¯”å£°éŸ³çŸ­çš„â€œæŠ¢è·‘â€ç°è±¡
                    let fps: Double = 30.0
                    let frameCount = ceil(targetDuration * fps)
                    let alignedDuration = frameCount / fps
                    
                    audioAssets.append(fileURL)
                    durations.append(alignedDuration)
                    
                    print("âœ… Step \(i): Audio \(String(format: "%.2f", rawAudioDuration))s -> Video Step \(String(format: "%.2f", alignedDuration))s")
                } else {
                    // Fallback
                    durations.append(4.0)
                }
            }
            
            // 2. åˆæˆè§†é¢‘
            print("ğŸ¬ Making video...")
            await exportUsingAssetWriter(steps: Array(steps.prefix(count)), durations: durations, audioURLs: audioAssets, completion: completion)
        }
    }
    
    // ğŸ¤ ä¸²è¡Œç”ŸæˆéŸ³é¢‘æ ¸å¿ƒé€»è¾‘
    private func generateAudioSerial(text: String, to url: URL) async {
        return await withCheckedContinuation { continuation in
            self.audioContinuation = continuation
            synthesizer.startSpeaking(text, to: url)
        }
    }
    
    // ğŸ§ å›è°ƒ
    func speechSynthesizer(_ sender: NSSpeechSynthesizer, didFinishSpeaking finishedSpeaking: Bool) {
        audioContinuation?.resume()
        audioContinuation = nil
    }
    
    // ğŸ¬ è§†é¢‘åˆæˆ
    private func exportUsingAssetWriter(steps: [TraceStepModel], durations: [Double], audioURLs: [URL], completion: @escaping (URL?) -> Void) async {
        let width = 1920
        let height = 1080
        let outputURL = FileManager.default.temporaryDirectory.appendingPathComponent("Trace_Tutorial_\(UUID().uuidString).mp4")
        
        try? FileManager.default.removeItem(at: outputURL)
        
        guard let writer = try? AVAssetWriter(outputURL: outputURL, fileType: .mp4) else {
            completion(nil); return
        }
        
        let videoSettings: [String: Any] = [
            AVVideoCodecKey: AVVideoCodecType.h264,
            AVVideoWidthKey: width,
            AVVideoHeightKey: height
        ]
        let videoInput = AVAssetWriterInput(mediaType: .video, outputSettings: videoSettings)
        let adaptor = AVAssetWriterInputPixelBufferAdaptor(assetWriterInput: videoInput, sourcePixelBufferAttributes: nil)
        
        if writer.canAdd(videoInput) { writer.add(videoInput) }
        
        writer.startWriting()
        writer.startSession(atSourceTime: .zero)
        
        let mediaInputQueue = DispatchQueue(label: "mediaInputQueue")
        
        await withCheckedContinuation { continuation in
            videoInput.requestMediaDataWhenReady(on: mediaInputQueue) {
                var frameTime = CMTime.zero
                let fps: Int32 = 30
                
                for (index, step) in steps.enumerated() {
                    let nsImage = step.image ?? NSImage(size: NSSize(width: width, height: height))
                    guard let cgImage = nsImage.cgImage(forProposedRect: nil, context: nil, hints: nil) else { continue }
                    
                    // è·å–å¯¹é½åçš„æ—¶é•¿
                    let durationSeconds = durations.count > index ? durations[index] : 4.0
                    
                    // è¿™é‡Œçš„ frameCount åº”è¯¥å’Œ createTutorialVideo é‡Œè®¡ç®—çš„ä¸€è‡´
                    let frameCount = Int(round(durationSeconds * Double(fps)))
                    
                    for _ in 0..<frameCount {
                        while !videoInput.isReadyForMoreMediaData { Thread.sleep(forTimeInterval: 0.01) }
                        
                        if let buffer = self.pixelBufferFromImage(cgImage, size: CGSize(width: width, height: height)) {
                            adaptor.append(buffer, withPresentationTime: frameTime)
                        }
                        frameTime = CMTimeAdd(frameTime, CMTime(value: 1, timescale: fps))
                    }
                }
                
                videoInput.markAsFinished()
                writer.finishWriting {
                    continuation.resume()
                }
            }
        }
        
        if writer.status == .failed {
            print("âŒ Video Writing Failed: \(String(describing: writer.error))")
            completion(nil)
            return
        }
        
        // ğŸ‰ åˆå¹¶éŸ³é¢‘
        if audioURLs.isEmpty {
            completion(outputURL)
        } else {
            await muxAudio(videoURL: outputURL, audioURLs: audioURLs, durations: durations, finalCompletion: completion)
        }
    }
    
    // ğŸ§ åˆå¹¶éŸ³é¢‘è½¨é“
    private func muxAudio(videoURL: URL, audioURLs: [URL], durations: [Double], finalCompletion: @escaping (URL?) -> Void) async {
        let mixComposition = AVMutableComposition()
        
        let videoAsset = AVURLAsset(url: videoURL)
        guard let videoTrack = try? await videoAsset.loadTracks(withMediaType: .video).first,
              let compositionVideoTrack = mixComposition.addMutableTrack(withMediaType: .video, preferredTrackID: kCMPersistentTrackID_Invalid) else {
            finalCompletion(videoURL); return
        }
        
        let videoDuration = try? await videoAsset.load(.duration)
        try? compositionVideoTrack.insertTimeRange(CMTimeRange(start: .zero, duration: videoDuration ?? .zero), of: videoTrack, at: .zero)
        
        if let compositionAudioTrack = mixComposition.addMutableTrack(withMediaType: .audio, preferredTrackID: kCMPersistentTrackID_Invalid) {
            var atTime = CMTime.zero
            
            for (index, audioURL) in audioURLs.enumerated() {
                guard FileManager.default.fileExists(atPath: audioURL.path) else { continue }
                
                let audioAsset = AVURLAsset(url: audioURL)
                if let track = try? await audioAsset.loadTracks(withMediaType: .audio).first {
                    let assetDuration = try? await audioAsset.load(.duration)
                    // æ’å…¥éŸ³é¢‘
                    try? compositionAudioTrack.insertTimeRange(CMTimeRange(start: .zero, duration: assetDuration ?? .zero), of: track, at: atTime)
                }
                
                // å…³é”®ï¼šå°†æ—¶é—´æŒ‡é’ˆå‘åç§»åŠ¨æ•´ä¸ª Step çš„æ—¶é•¿ï¼ˆéŸ³é¢‘ + 1sï¼‰
                // è¿™æ ·ä¸‹ä¸€æ®µéŸ³é¢‘å°±ä¼šåœ¨å›¾ç‰‡åˆ‡æ¢çš„åŒæ—¶å¼€å§‹æ’­æ”¾
                let stepDuration = CMTime(seconds: durations[index], preferredTimescale: 600)
                atTime = CMTimeAdd(atTime, stepDuration)
            }
        }
        
        let finalURL = FileManager.default.temporaryDirectory.appendingPathComponent("Trace_Final_\(UUID().uuidString).mp4")
        guard let exportSession = AVAssetExportSession(asset: mixComposition, presetName: AVAssetExportPreset1920x1080) else {
            finalCompletion(videoURL); return
        }
        
        exportSession.outputURL = finalURL
        exportSession.outputFileType = .mp4
        await exportSession.export()
        
        if exportSession.status == .completed {
            print("âœ… Video export success: \(finalURL)")
            finalCompletion(finalURL)
        } else {
            print("âŒ Video export failed: \(String(describing: exportSession.error))")
            finalCompletion(videoURL)
        }
    }
    
    // è¾…åŠ©ï¼šPixelBuffer
    private func pixelBufferFromImage(_ image: CGImage, size: CGSize) -> CVPixelBuffer? {
        var pxbuffer: CVPixelBuffer?
        let options: [String: Any] = [
            kCVPixelBufferCGImageCompatibilityKey as String: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey as String: true
        ]
        CVPixelBufferCreate(kCFAllocatorDefault, Int(size.width), Int(size.height), kCVPixelFormatType_32ARGB, options as CFDictionary, &pxbuffer)
        guard let buffer = pxbuffer else { return nil }
        
        CVPixelBufferLockBaseAddress(buffer, [])
        let pxdata = CVPixelBufferGetBaseAddress(buffer)
        let rgbColorSpace = CGColorSpaceCreateDeviceRGB()
        let context = CGContext(data: pxdata, width: Int(size.width), height: Int(size.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(buffer), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue)
        
        context?.setFillColor(NSColor.black.cgColor)
        context?.fill(CGRect(x: 0, y: 0, width: size.width, height: size.height))
        
        let imageWidth = CGFloat(image.width)
        let imageHeight = CGFloat(image.height)
        let aspect = imageWidth / imageHeight
        let targetAspect = size.width / size.height
        var drawRect = CGRect.zero
        
        if aspect > targetAspect {
            let newHeight = size.width / aspect
            drawRect = CGRect(x: 0, y: (size.height - newHeight) / 2, width: size.width, height: newHeight)
        } else {
            let newWidth = size.height * aspect
            drawRect = CGRect(x: (size.width - newWidth) / 2, y: 0, width: newWidth, height: size.height)
        }
        
        context?.draw(image, in: drawRect)
        CVPixelBufferUnlockBaseAddress(buffer, [])
        return buffer
    }
}
